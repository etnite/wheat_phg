{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression Recursive Feature Elimination\n",
    "\n",
    "Brian Ward  \n",
    "brian@brianpward.net  \n",
    "https://github.com/etnite\n",
    "\n",
    "This notebook is one attempt to create a method to identify a minimal set of predictors (SNPs) which fully describe a non-ordinal categorical response (haplotypes) for a given region of the genome. My idea is to perform a (possibly multinomial) logistic regression, using recursive feature elimination (RFE) to select a good subset of SNPs\n",
    "\n",
    "There are many different methods for finding \"tag SNPs\" which can capture the variance explained by multiple surrounding SNPs. This problem is non-trivial and potentially NP-hard. See https://doi.org/10.1109/CSB.2005.22 for one example and some background on different methods. Note that much of this work was performed around the time of the first human HapMap project (ca. 2005), and hence there are many links to software on websites which no longer exist. These programs are also likely to be written in C or Perl, and getting them to run and use current data formats can be very difficult.\n",
    "\n",
    "Many tag SNP identification algorithms are designed to find sets of tag SNPs across chromosomes/genomes, without any phenotypic data. I think that my particular use-case here is actually somewhat simpler, since I:\n",
    "\n",
    "1. Have access to a \"phenotype\" (haplotype groups determined by a simple distance matrix using all SNPs/variants in the region of interest)\n",
    "2. Only need to focus on a single region, obviating the need to find haploblock boundaries\n",
    "\n",
    "This script performs repeated k-fold validation with the full set of SNPs in a region, with the selected subset of *n* SNPs identified by scikit-learn's RFE algorithm, and with the first *n* principal components of the full SNP matrix. We use the handy scikit-allel module to import data from a VCF file http://alimanfoo.github.io/2017/06/14/read-vcf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn import preprocessing\n",
    "import allel\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Constants\n",
    "\n",
    "Constants are as follows:\n",
    "\n",
    "* Path to VCF (genotypic data) file\n",
    "* Region of interest formatted chr:start_pos-end_pos\n",
    "* .csv file of haplotypes - must contain at least two cols. - one for sample, one for the haplotype groupings\n",
    "* response - string identifying the response variable in the haplotypes file\n",
    "* n_snps - Desired number of SNPs to retain\n",
    "* n_reps - Number of times to repeat k-fold validation\n",
    "* val_k - Number of folds per repetition of k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_file = '/home/brian/repos/manuscripts/manu_2018_stripe_rust/input_data/geno/GAWN_Yr_postimp_filt_KASP.vcf.gz'\n",
    "reg = '4B:526943215-598847646'\n",
    "haps_file = '/home/brian/Downloads/haplotype_groupings.csv'\n",
    "response = '4BL'\n",
    "min_snps = 4\n",
    "max_snps = 5\n",
    "n_reps = 5\n",
    "val_k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input\n",
    "\n",
    "Here we read in the region of interest from the VCF file. We then convert the GT calls from the VCF into a genotype array (this is an intermediate step, and I'm currently not sure if it is required or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/DATAPART2/brian/repos/manuscripts/manu_2020_Yr/input_data/geno/GAWN_KM_Yr_postimp_filt.vcf.gz'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/allel/io/vcf_read.py\u001b[0m in \u001b[0;36m_setup_input_stream\u001b[0;34m(input, region, tabix, buffer_size)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m                 \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [E::hts_open_format] Failed to open file /mnt/DATAPART2/brian/repos/manuscripts/manu_2020_Yr/input_data/geno/GAWN_KM_Yr_postimp_filt.vcf.gz\nCould not read /mnt/DATAPART2/brian/repos/manuscripts/manu_2020_Yr/input_data/geno/GAWN_KM_Yr_postimp_filt.vcf.gz",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-840330960fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_vcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcf_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvcf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'variants/ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenotypeArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'calldata/GT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Samples:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'samples'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of SNPs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/allel/io/vcf_read.py\u001b[0m in \u001b[0;36mread_vcf\u001b[0;34m(input, fields, exclude_fields, rename_fields, types, numbers, alt_number, fills, region, tabix, samples, transformers, buffer_size, chunk_length, log)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;31m# setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     fields, samples, headers, it = iter_vcf_chunks(\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mnumbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/allel/io/vcf_read.py\u001b[0m in \u001b[0;36miter_vcf_chunks\u001b[0;34m(input, fields, exclude_fields, types, numbers, alt_number, fills, region, tabix, samples, transformers, buffer_size, chunk_length)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[0;31m# setup input stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m     stream = _setup_input_stream(input=input, region=region, tabix=tabix,\n\u001b[0m\u001b[1;32m   1144\u001b[0m                                  buffer_size=buffer_size)\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/allel/io/vcf_read.py\u001b[0m in \u001b[0;36m_setup_input_stream\u001b[0;34m(input, region, tabix, buffer_size)\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 warnings.warn('error occurred attempting tabix (%s); falling back to '\n\u001b[1;32m   1057\u001b[0m                               'scanning to region' % e)\n\u001b[0;32m-> 1058\u001b[0;31m                 \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m                 \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/DATAPART2/brian/repos/manuscripts/manu_2020_Yr/input_data/geno/GAWN_KM_Yr_postimp_filt.vcf.gz'"
     ]
    }
   ],
   "source": [
    "vcf = allel.read_vcf(vcf_file, region = reg)\n",
    "preds = vcf['variants/ID']\n",
    "gt = allel.GenotypeArray(vcf['calldata/GT'])\n",
    "print(\"Number of Samples:\", len(vcf['samples']))\n",
    "print(\"Number of SNPs:\", len(preds))\n",
    "gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the genotype array into minor allele dosage format, then convert to a dataframe, set SNP IDs as col. names, and add a column for the sample name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos = gt.to_n_alt()\n",
    "dos = dos.transpose()\n",
    "\n",
    "dos_df = pd.DataFrame(dos)\n",
    "dos_df.columns = vcf[\"variants/ID\"]\n",
    "dos_df[\"sample\"] = vcf[\"samples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the haplotypes file, merge it with the dos_df dataframe, and set the response vector (y) and predictors matrix (X). We then standardize each predictor to form the X_std matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.14019606, -0.71251567, -0.33202381, ...,  1.00469512,\n",
       "         1.01349415,  1.36287906],\n",
       "       [ 0.92052526, -0.71251567, -0.33202381, ..., -1.01701007,\n",
       "        -1.01555409, -0.74952987],\n",
       "       [ 0.92052526, -0.71251567, -0.33202381, ..., -1.01701007,\n",
       "        -1.01555409, -0.74952987],\n",
       "       ...,\n",
       "       [-1.14019606, -0.71251567, -0.33202381, ...,  1.00469512,\n",
       "         1.01349415,  1.36287906],\n",
       "       [ 0.92052526, -0.71251567, -0.33202381, ...,  1.00469512,\n",
       "         1.01349415, -0.74952987],\n",
       "       [ 0.92052526,  1.45361993, -0.33202381, ..., -1.01701007,\n",
       "        -1.01555409, -0.74952987]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haps = pd.read_csv(haps_file)\n",
    "haps = haps[[\"sample\", response]]\n",
    "\n",
    "merged = pd.merge(haps, dos_df, how = \"inner\", on = \"sample\")\n",
    "y = merged[\"4BL\"]\n",
    "X = merged[vcf[\"variants/ID\"]]\n",
    "\n",
    "\n",
    "sc = preprocessing.StandardScaler()\n",
    "X_std = sc.fit_transform(X)\n",
    "X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Output Structures\n",
    "\n",
    "Here we need to create a list of dataframes to store the output statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_df = 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Feature Elimination\n",
    "\n",
    "Here we create a multinomial logistic regressor. Then we utilize recursive feature elimination (RFE) to try and remove non-informative/repetitive features (i.e. SNPs) until we get down to the number of SNPs chosen by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlogit = LogisticRegression(solver = 'lbfgs', penalty = 'none', multi_class = 'auto', max_iter = 5e3)\n",
    "rfe = RFE(mlogit, n_snps)\n",
    "fit = rfe.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained features: ['S4B_546015349' 'S4B_546664811' 'S4B_592005439' 'S4B_596498722']\n",
      "Retained features array first 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.97658556, -0.979656  , -0.82095677, -0.84629444],\n",
       "       [ 1.05067846,  1.05398185, -0.82095677, -0.84629444],\n",
       "       [ 1.05067846,  1.05398185, -0.82095677, -0.84629444],\n",
       "       [ 1.05067846,  1.05398185,  1.24718241,  1.20690759],\n",
       "       [-0.97658556, -0.979656  ,  1.24718241,  1.20690759]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_preds = preds[fit.support_]\n",
    "print(\"Retained features:\", sub_preds)\n",
    "X_sub = X_std[:, fit.support_]\n",
    "\n",
    "print(\"Retained features array first 5 rows:\")\n",
    "X_sub[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "We can also perform logistic regression using the same number of principal components as the number of SNPs we decided to use. This should give us a theoretical maximum predicted vs. observed haplotypes accuracy for a given number n of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnother variation - find the SNPs most highly correlated with each PC\\nloadings = np.abs(pca.components_)\\ntop_corrs = np.argmax(loadings, axis = 1)\\nX_pcs = X_std[:, top_corrs]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components = n_snps)\n",
    "pca.fit(np.transpose(X_std))\n",
    "X_pcs = pca.components_.T\n",
    "\n",
    "'''\n",
    "Another variation - find the SNPs most highly correlated with each PC\n",
    "loadings = np.abs(pca.components_)\n",
    "top_corrs = np.argmax(loadings, axis = 1)\n",
    "X_pcs = X_std[:, top_corrs]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-validation\n",
    "\n",
    "Now we perform n repeats of k-fold cross-validation, and find the mean accuracy of the prediction (proportion of times the predicted response category matches the observed category). We do this for the full set of SNPs, the chosen subset of *n* SNPs, and the first *n* principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_score_arr = np.zeros((n_reps, val_k), dtype = np.float)\n",
    "for i in range(0, n_reps):\n",
    "    k_fold = KFold(n_splits = val_k, random_state = i, shuffle = True)\n",
    "    full_score_arr[i,:] = cross_val_score(mlogit, X_std, y, cv = k_fold, scoring = 'accuracy')\n",
    "\n",
    "sub_score_arr = np.zeros((n_reps, val_k), dtype = np.float)\n",
    "for i in range(0, n_reps):\n",
    "    k_fold = KFold(n_splits = val_k, random_state = i, shuffle = True)\n",
    "    sub_score_arr[i,:] = cross_val_score(mlogit, X_sub, y, cv = k_fold, scoring = 'accuracy')\n",
    "\n",
    "pc_score_arr = np.zeros((n_reps, val_k), dtype = np.float)\n",
    "for i in range(0, n_reps):\n",
    "    k_fold = KFold(n_splits = val_k, random_state = i, shuffle = True)\n",
    "    pc_score_arr[i,:] = cross_val_score(mlogit, X_pcs, y, cv = k_fold, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy, all SNPs:\n",
      "0.98\n",
      "\n",
      "Prediction accuracy, selected SNPs:\n",
      "0.85\n",
      "\n",
      "Prediction accuracy, PCs:\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction accuracy, all SNPs:\")\n",
    "print(round(np.mean(full_score_arr), 2))\n",
    "\n",
    "print(\"\\nPrediction accuracy, selected SNPs:\")\n",
    "print(round(np.mean(sub_score_arr), 2))\n",
    "\n",
    "print(\"\\nPrediction accuracy, PCs:\")\n",
    "print(round(np.mean(pc_score_arr), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this case, it appears that there is no small subset of SNPs that have very a high predictive accuracy when compared against the ~100% prediction accuracy using all 427 SNPs used to identify the haplotypes in the first place. Notably, using just the first handful of principal components of the genotypic matrix, we can almost perfectly predict the haplotypes. However, an alternative strategy, which was to use the SNPs most highly correlated with each PC, yielded similar results the RFE method (mean accuracy of ~ 0.85, method not shown here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}